{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Iris.csv\")\n",
        "\n",
        "# Convert Categorical into Numerical\n",
        "df[\"Species\"].replace([\"Iris-setosa\",\"Iris-versicolor\",\"Iris-virginica\"],[0,1,2],inplace=True)\n",
        "\n",
        "# Assign Features and Label\n",
        "Y = df[\"Species\"].to_numpy()\n",
        "X = df.drop([\"Id\",\"Species\"],axis=1).to_numpy()\n",
        "\n",
        "# Split Dataset into Train and Test Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=720)\n",
        "\n",
        "# Create External Functions\n",
        "def affine_forward(X, weight, bias):\n",
        "    out = np.dot(X,weight.T) + bias.T\n",
        "\n",
        "    ldx = weight\n",
        "    ldw = X\n",
        "    ldb = 1\n",
        "    return out,ldx,ldw,ldb\n",
        "\n",
        "def affine_backward(ud, ldx, ldw, ldb):\n",
        "    dx = np.dot(ud,ldx)\n",
        "    dw = np.dot(ud.T,ldw)\n",
        "    db = np.sum(ud,axis=0).reshape(-1, 1)\n",
        "    return dx,dw,db\n",
        "\n",
        "def gradient_decent(weight,der,alpha):\n",
        "\n",
        "  return weight - alpha * der\n",
        "\n",
        "def SVMloss_Func(logits,Y):\n",
        "\n",
        "  diff = logits - logits[np.arange(Y.shape[0]),Y].reshape(Y.shape[0], 1)\n",
        "  diff[diff < 0] = 0\n",
        "  loss = np.sum(diff)\n",
        "  diff[diff > 0 ] = 1\n",
        "  diff[np.arange(Y.shape[0]), Y] = -1 * np.sum(diff, axis=1)\n",
        "  return loss, diff\n",
        "\n",
        "def relu_forward(X):\n",
        "    X[X<0] = 0\n",
        "    dx = np.zeros(X.shape)\n",
        "    dx[X>0] = 1\n",
        "    return X,dx\n",
        "\n",
        "def relu_backward(ud, ld):\n",
        "    return ud * ld\n",
        "\n",
        "# Create Neural Network Class\n",
        "class SingleLayerNeuralNetwork:\n",
        "    def _init_(self, num_layers, neurons_per_layer):\n",
        "        self.num_layers = num_layers\n",
        "        self.neurons_per_layer = neurons_per_layer\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    # Create a Train Function\n",
        "    def train(self, X, Y, alpha, max_no_iter, loss_iter):\n",
        "        # Initialize random weights and biases\n",
        "        self.weights = []\n",
        "        self.bias = []\n",
        "        for i in range(self.num_layers):\n",
        "            if i < self.num_layers - 1:\n",
        "                weight = np.random.randn(self.neurons_per_layer[i], X.shape[1])\n",
        "                bias = np.zeros((self.neurons_per_layer[i], 1))\n",
        "            else:\n",
        "                # For the output layer\n",
        "                weight = np.random.randn(self.neurons_per_layer[i], self.neurons_per_layer[i-1])\n",
        "                bias = np.zeros((self.neurons_per_layer[i], 1))\n",
        "            self.weights.append(weight)\n",
        "            self.bias.append(bias)\n",
        "        for j in range(max_no_iter):\n",
        "            logits = [X]\n",
        "            ldx = []\n",
        "            ldw = []\n",
        "            ldb = []\n",
        "            relu_dx = []\n",
        "            # Forward pass (calculate logits)\n",
        "            for i in range(self.num_layers):\n",
        "                forward_out, ld_x, ld_w, ld_b = affine_forward(logits[-1],self.weights[i],self.bias[i])\n",
        "                ldx.append(ld_x)\n",
        "                ldw.append(ld_w)\n",
        "                ldb.append(ld_b)\n",
        "                relu_out, r_dx = relu_forward(forward_out)\n",
        "                relu_dx.append(r_dx)\n",
        "                logits.append(relu_out)\n",
        "\n",
        "            # Calculate Loss\n",
        "            loss , der  =  SVMloss_Func( logits[-1], Y)\n",
        "            if (j + 1) % loss_iter == 0:\n",
        "                # Print Loss Value\n",
        "                print(\"After \", j+1 ,\"Iteration, Loss: \",loss)\n",
        "\n",
        "            # Backward pass\n",
        "            backward_derivatives = [der]\n",
        "            for i in range(self.num_layers - 1, -1, -1):\n",
        "                #print(backward_derivatives[-1].shape)\n",
        "                #print(\"Der\",relu_dx[i].shape)\n",
        "                der = relu_backward(backward_derivatives[-1], relu_dx[i])\n",
        "                dx, dw, db = affine_backward(der, ldx[i], ldw[i], ldb[i])\n",
        "                backward_derivatives.append(dx)\n",
        "                # Update weights and biases\n",
        "                self.weights[i] = gradient_decent(self.weights[i],dw,alpha)\n",
        "                self.bias[i] = gradient_decent(self.bias[i],db,alpha)\n",
        "\n",
        "    # Create a Test Function\n",
        "    def test(self, X):\n",
        "        logits = [X]\n",
        "        ldx = []\n",
        "        ldw = []\n",
        "        ldb = []\n",
        "        relu_dx = []\n",
        "        for i in range(self.num_layers):\n",
        "            forward_out, ld_x, ld_w, ld_b = affine_forward(logits[-1], self.weights[i], self.bias[i])\n",
        "            ldx.append(ld_x)\n",
        "            ldw.append(ld_w)\n",
        "            ldb.append(ld_b)\n",
        "            relu_out, r_dx = relu_forward(forward_out)\n",
        "            relu_dx.append(r_dx)\n",
        "            logits.append(relu_out)\n",
        "\n",
        "        predicted_classes = np.argmax(relu_out, axis=1)\n",
        "        return predicted_classes\n",
        "\n",
        "# Create Object of the Neural Network class\n",
        "SNn = SingleLayerNeuralNetwork(3, [4, 5, 3])\n",
        "\n",
        "# Train the class\n",
        "SNn.train(X_train, y_train, 0.00001, 1000, 1)\n",
        "\n",
        "# Test the trained model\n",
        "predictions = SNn.test(X_test)\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "wtaRJaFT3KPd",
        "outputId": "5b76f806-0ba3-45cd-970c-2cd2d507f5bc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "SingleLayerNeuralNetwork() takes no arguments",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-732624ac2a97>\u001b[0m in \u001b[0;36m<cell line: 133>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;31m# Create Object of the Neural Network class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m \u001b[0mSNn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleLayerNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;31m# Train the class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: SingleLayerNeuralNetwork() takes no arguments"
          ]
        }
      ]
    }
  ]
}